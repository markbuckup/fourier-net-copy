import torch
import torchvision
import os
import random
import PIL
import argparse
import sys
sys.path.append('../../')
from tqdm import tqdm
import kornia
import pickle
from kornia.metrics import ssim
from torchvision import transforms
import matplotlib.pyplot as plt
import kornia
from kornia.utils import draw_line
import numpy as np
import nibabel as nib
from skimage.exposure import match_histograms

import torchkbnufft as tkbn

from utils.functions import get_window_mask, get_coil_mask

def seed_torch(seed=0):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True
seed_torch(0)

parser = argparse.ArgumentParser()
parser.add_argument('--resolution', type = int, default = 256)
parser.add_argument('--gpu', type = int, default = -1)
parser.add_argument('--pat_start', type = int, default = 1)
parser.add_argument('--pat_end', type = int, default = 150)
parser.add_argument('--save_path', type = str, default = '/Data/ExtraDrive1/niraj/datasets/ACDC')
parser.add_argument('--metadata_only', action = 'store_true')
args = parser.parse_args()

if args.gpu == -1:
    device = torch.device('cpu')
else:
    device = torch.device('cuda:{}'.format(args.gpu))

histogram_target = nib.load('raw/patient001/patient001_4d.nii.gz').get_fdata()[:,:,0,7]

def read_nib_preprocess(path, heq = True):
    img = nib.load(path).get_fdata()
    drop_videos = []
    flag = False
    tot = 0
    for vi in range(img.shape[2]):
        if (not np.isfinite(img[:,:,vi,:]).all()) or (img[:,:,vi,:].sum(0).sum(0) == 0).any():
            # drop video vi
            drop_videos.append(vi)
            flag = True
    for di in reversed(sorted(drop_videos)):
        tot += 1
        img = np.concatenate((img[:,:,:di,:], img[:,:,di+1:,:]), 2)
    if heq:
        for i in range(img.shape[2]):
            for j in range(img.shape[3]):
                img[:,:,i,j] = match_histograms(img[:,:,i,j], histogram_target)
                img[:,:,i,j] = img[:,:,i,j] - img[:,:,i,j].min()
                img[:,:,i,j] = img[:,:,i,j] / img[:,:,i,j].max()
    return img, flag, tot

def num_to_str(num):
    x = str(num)
    while len(x) < 3:
        x = '0' + x
    return './raw/patient{}/patient{}_4d.nii.gz'.format(x,x)

GR = (1 + (5**0.5))/2
GA = np.pi/GR

NUM_PATIENTS = 150
RES=args.resolution
NUM_SPOKES = 10
N_COILS_VARIANTS = 100
N_COILS = 8
LOOP_FRAMES = 120

# Define the Gridding and reverse gridding operations
kb_ob = tkbn.KbInterp(im_size=(512,512), grid_size = (1024,1024), numpoints = 6, kbwidth = 19.34, device = device)
kbinterp = tkbn.KbInterpAdjoint(im_size=(256,256), grid_size = (256,256), numpoints = 3, kbwidth = 2.34, device = device)
recon_im_size = (256,256)
recon_grid_size = (1024,1024)
recon_num_points = 8
recon_kbwidth = 0.84
kbinterp2 = tkbn.KbInterpAdjoint(im_size=recon_im_size, grid_size = recon_grid_size, numpoints = recon_num_points, kbwidth = recon_kbwidth, device = device)
kbinterp_full = tkbn.KbInterpAdjoint(im_size=recon_im_size, grid_size = recon_grid_size, numpoints = 6, kbwidth = 9.34, device = device)


def get_omega2d(thetas, rad_max = 128):
    """
    Generates a 2D frequency grid for given angles (thetas) and a specified maximum radius (rad_max).

    Parameters:
    thetas (Tensor): A tensor of angles (in radians) for which the frequency grid will be generated.
    rad_max (int, optional): The maximum radius value for the frequency grid. Default is 128.

    Returns:
    Tensor: A tensor of shape (2, len(thetas), 2*rad_max+1) containing the frequency coordinates.
            The first dimension represents the x and y coordinates, the second dimension corresponds
            to the provided angles (thetas), and the third dimension contains the frequency values
            ranging from -pi to pi.

    The frequency grid (omega1) is generated by calculating the x and y coordinates based on the radii
    and angles, normalizing these coordinates by rad_max, and scaling by pi.

    Example:
    thetas = torch.tensor([0, np.pi/4, np.pi/2])
    omega1 = get_omega2d(thetas)
    """    
    omega1 = torch.zeros(2,thetas.shape[0],2*rad_max+1).to(device)

    for itheta, theta_rad in enumerate(thetas):

        rads = np.arange(-rad_max,rad_max+1)
        xs = rads*np.cos(theta_rad)
        ys = rads*np.sin(theta_rad)
        
        omega1[0,itheta,:] = torch.from_numpy(np.pi*(xs/rad_max)) # ?, in 0 axis omega, filled with xs, rad_max=256, every omega value will be between 0 and 1
        omega1[1,itheta,:] = torch.from_numpy(np.pi*(ys/rad_max)) # ?, in 1 axis omega, filled with ys
    return omega1


# Generate the omegas for a fully sampled image - will be used as ground truth
angles = GA*np.arange(1000)
omega_full = get_omega2d(angles, rad_max = 2048).to(device).reshape(2,-1)

# Generate the density compensation for this fully sampled kspace
dcomp_full = tkbn.calc_density_compensation_function(ktraj=omega_full, im_size=recon_im_size, grid_size = recon_grid_size, numpoints = 6, kbwidth = 9.34).to(device)

# Transformation to reduce resolution
# Any resolution less than 256 should be cropped to ensure the weights do not learn a shrinked representation
transform = torch.nn.Sequential(
    transforms.Resize((256,256), antialias = True),
    transforms.CenterCrop(RES),
)


# Write a metadictionary with the following attributes
# metadic['num_patients']                           = Number of Patients
# metadic['coil_masks']                             = A matrix of size (N_COILS_VARIANTS,N_COILS, RES, RES)
#                                                           Each variant is a set of Coils - gaussian masks with their centres scattered
#                                                           Each variant is slightly rotated than the others
# metadic['coil_variant_per_patient_per_video']     = We assign each patient and video a paritcular coil variant randomly - we store in the metadic to ensure consistency if we rerun the script
# metadic['GAs_per_patient_per_video']              = We assign each frame of (patient and video) a set of acquisition angles - we store in the metadic to ensure consistency if we rerun the script
# metadic['num_vids_per_patient']                   = Number of slices for each patient - list of lists
# metadic['frames_per_vid_per_patient']             = Number of frames / Cardiac phases for each patient (each slice must have the same number of cardiac phases)
metapath = 'radial_faster/{}_resolution_{}_spokes/metadata.pth'.format(RES, NUM_SPOKES)
if os.path.isfile(metapath):
    metadic = torch.load(metapath)
    print("Meta Data Loaded")
else:
    os.makedirs(os.path.join(args.save_path, 'radial_faster/{}_resolution_{}_spokes'.format(RES, NUM_SPOKES)), exist_ok = True)
    metadic = {}
    metadic['num_patients'] = NUM_PATIENTS
    metadic['coil_masks'] = torch.zeros((N_COILS_VARIANTS,N_COILS, RES, RES))
    for i in range(N_COILS_VARIANTS):
        metadic['coil_masks'][i] = get_coil_mask(theta_init = (np.pi/4)*i*(1/N_COILS_VARIANTS), n_coils = N_COILS, resolution = RES)

    metadic['coil_variant_per_patient_per_video'] = [[] for i in range(NUM_PATIENTS)]

    metadic['GAs_per_patient_per_video'] = [[] for i in range(NUM_PATIENTS)]
    metadic['num_vids_per_patient'] = [0 for i in range(NUM_PATIENTS)]
    metadic['frames_per_vid_per_patient'] = [0 for i in range(NUM_PATIENTS)]
    

    for p_num in tqdm(range(NUM_PATIENTS), desc = 'Writing Metadata patient data'):
        path = num_to_str(p_num+1)
        d, flag, dvid = read_nib_preprocess(path)
        if flag:
            print("Patient {} flagged, {} video(s) dropped".format(i, dvid))
        d = torch.permute(torch.FloatTensor(d).unsqueeze(4), (2,3,4,0,1))
        num_vids, num_frames, n_coils, n_row, n_col = d.shape

        metadic['num_vids_per_patient'][p_num] = num_vids
        metadic['frames_per_vid_per_patient'][p_num] = num_frames

        for vi in range(num_vids):
            ga_random_start = np.random.randint(377)
            metadic['GAs_per_patient_per_video'][p_num].append((ga_random_start+np.arange(LOOP_FRAMES*NUM_SPOKES).reshape(LOOP_FRAMES,NUM_SPOKES))%377)
            if not metadic['coil_variant_per_patient_per_video'][p_num]:
                metadic['coil_variant_per_patient_per_video'][p_num] = [0 for i in range(num_vids)]
            metadic['coil_variant_per_patient_per_video'][p_num][vi] = np.random.randint(N_COILS_VARIANTS)

    torch.save(metadic, metapath)
    print("Meta Data Generated")


if not args.metadata_only:
    for p_num in range(args.pat_start-1, args.pat_end):
        # Create the folder to save the processed video
        folder_name = os.path.join(args.save_path, 'radial_faster/{}_resolution_{}_spokes/patient_{}'.format(RES, NUM_SPOKES,p_num+1))
        os.makedirs(folder_name, exist_ok = True)

        path = num_to_str(p_num+1)
        # Read in the video
        d_inp, flag, dvid = read_nib_preprocess(path)
        if flag:
            print("Patient {} flagged, {} video(s) dropped".format(p_num, dvid))
        d_inp = torch.permute(torch.FloatTensor(d_inp).unsqueeze(4), (2,3,4,0,1))
        num_vids, num_frames, n_coils, n_row, n_col = d_inp.shape # Reshape
        # Pad the image with zeros to create a square image
        d_padded = torch.zeros(num_vids, num_frames, n_coils, max(n_row, n_col),max(n_row, n_col))
        row_pad = (max(n_row, n_col) - n_row)//2
        col_pad = (max(n_row, n_col) - n_col)//2
        d_padded[:,:,:,row_pad:row_pad + n_row, col_pad:col_pad+n_col] = d_inp
        del d_inp

        # Center crop the image according to the required resoltuion and scale it 0-1
        d = transform(d_padded.reshape(num_vids*num_frames, 1, max(n_row,n_col), max(n_row,n_col))).reshape(num_vids, num_frames, n_coils, RES, RES)
        d = d - d.min(-1)[0].min(-1)[0].unsqueeze(-1).unsqueeze(-1)
        d = d / (1e-10 + d.max(-1)[0].max(-1)[0].unsqueeze(-1).unsqueeze(-1))


        for vi in tqdm(range(num_vids), desc = 'Running for Patient {}'.format(p_num+1)):
            dic = {}
            dic['targ_video'] = d[vi,np.arange(LOOP_FRAMES)%num_frames] # FRAMES, 1, RES, RES
            dic['spoke_mask'] = torch.zeros((LOOP_FRAMES, 1, RES, RES)).type(torch.uint8) # FRAMES, COILS, RES, RES
            # dic['coilwise_input_mag'] = torch.zeros((LOOP_FRAMES, N_COILS, RES, RES)).type(torch.uint8) # FRAMES, COILS, RES, RES
            dic['coilwise_input'] = torch.zeros((LOOP_FRAMES, N_COILS, RES, RES)) # FRAMES, COILS, RES, RES
            dic['coilwise_input'] = torch.complex(dic['coilwise_input'],dic['coilwise_input'])
            dic['coilwise_target'] = torch.zeros((LOOP_FRAMES, N_COILS, RES, RES)).type(torch.uint8) # FRAMES, COILS, RES, RES

            for fi in range(LOOP_FRAMES):

                # Fetch the Coil Variant from the metadictionary
                coil_variant = metadic['coil_variant_per_patient_per_video'][p_num][vi]
                curr_coil = metadic['coil_masks'][coil_variant]

                # Get the coilwise ground truth
                input_frame = dic['targ_video'][fi,:,:,:] * curr_coil

                # Important! The kspace data should be such that the sum of squares of the input is in the range (0,1)
                temp = ((input_frame**2).sum(0)**0.5)
                input_frame = input_frame / (1e-10 + temp.max())

                # Fetch the sampled angles from the metadictionary
                angle_indices = metadic['GAs_per_patient_per_video'][p_num][vi][fi]
                angles = GA*angle_indices

                # Generate the omegas - x,y coordinates 
                omega1 = get_omega2d(angles, rad_max = 2048).to(device).reshape(2,-1)

                # Pad the image with zeros to prevent artefacts and the the fft
                padded_frame = torch.zeros(1, N_COILS, 2*input_frame.shape[1],2*input_frame.shape[2])
                padded_frame[0,:,(RES//2):(RES//2)+RES,(RES//2):(RES//2)+RES] = input_frame
                padded_frame = torch.fft.fft2(padded_frame)
                
                # Reverse Gridding operation - obtain data at non grid coordiates
                y = kb_ob(padded_frame.to(device),omega1)
                # Do a similar operation for a fully sampled image - 1000 spokes
                y_full = kb_ob(padded_frame.to(device),omega_full)

                # compute the density compensation weights for the undersampled spokes
                dcomp_under = tkbn.calc_density_compensation_function(ktraj=omega1, im_size=recon_im_size, grid_size = recon_grid_size, numpoints = recon_num_points, kbwidth = recon_kbwidth).to(device)
                # If we grid the density compensation weights and threshold them, we get the mask of the spokes
                dic['spoke_mask'][fi,0,:,:] = torch.fft.fftshift(kbinterp(dcomp_under, omega1)[0,:].abs().squeeze(), dim = (-2,-1)).cpu() > 0.1

                # Gridding operation for both the undersampled and fully sampled image
                myfft_interp = torch.fft.fftshift(kbinterp2(dcomp_under*y, omega1)[0,:].cpu(), dim = (-2,-1))
                myfft_interp_full = torch.fft.fftshift(kbinterp_full(dcomp_full*y_full, omega_full)[0,:].cpu(), dim = (-2,-1))

                
                # Crop the image
                myfft_interp = myfft_interp[:,::4,::4]
                myfft_interp_full = myfft_interp_full[:,::4,::4]


                # Scale both the undersampled and fully sampled inputs so that the sum of squares of the ifft are in the range (0,1)                
                temp = torch.fft.ifft2(torch.fft.ifftshift(myfft_interp_full, dim = (-2,-1)))
                temp = temp - temp.abs().min(2)[0].min(1)[0].unsqueeze(-1).unsqueeze(-1)
                scaling_factor = 1/(1e-10 + ((temp.abs()**2).sum(0)**0.5).max())
                temp = temp * scaling_factor
                # temp now has k-space data such that the sum of squares of the ifft are in the range(0,1)
                temp = torch.fft.fftshift(torch.fft.fft2(temp), dim = (-2,-1))
                
                # Store the DC component and temporarily set it to 0
                true_dc = temp[:,RES//2,RES//2].clone()
                temp[:,RES//2,RES//2] = 0
                myfft_interp_full[:,RES//2,RES//2] = 0
                myfft_interp[:,RES//2,RES//2] = 0

                # Normalise the interpolated FFTs such that the mean of the 3x3 region near their center matches temp (computed above)
                myfft_interp_full = myfft_interp_full * (temp.abs()[:,127:130,127:130].mean(-2, keepdim = True).mean(-1, keepdim = True) / (myfft_interp_full.abs()[:,127:130,127:130].mean(-2, keepdim = True).mean(-1, keepdim = True) + 1e-10))
                myfft_interp = myfft_interp * (temp.abs()[:,127:130,127:130].mean(-2, keepdim = True).mean(-1, keepdim = True) / (myfft_interp.abs()[:,127:130,127:130].mean(-2, keepdim = True).mean(-1, keepdim = True) + 1e-10))
                # Restore the DC values according to temp (computed above)
                myfft_interp_full[:,RES//2,RES//2] = true_dc
                myfft_interp[:,RES//2,RES//2] = true_dc

                # Store the complex coilwise undersampled input  
                dic['coilwise_input'][fi] = myfft_interp
                # Store coilwise targets - storing as uint8 to conserve memory
                dic['coilwise_target'][fi] = (torch.fft.ifft2(torch.fft.ifftshift(myfft_interp_full, dim = (-2,-1))).abs().clip(0,1)*255).type(torch.uint8)

            # Store the target (ground truth) - storing as uint8 to conserve memory
            dic['targ_video'] = (dic['targ_video']*255).type(torch.uint8)
            torch.save(dic, os.path.join(folder_name, 'vid_{}.pth'.format(vi)))
